{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATest Experiments for ICTSS 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import OPTICS, DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "X = pd.read_csv('data/data.csv')\n",
    "\n",
    "# Dropping irrelevant rows where 'SimplifiedSCTUnitStatus' is not 'PASSED' or 'FAILED'\n",
    "X = X[(X['SimplifiedSCTUnitStatus'] == 'PASSED') | (X['SimplifiedSCTUnitStatus'] == 'FAILED')]\n",
    "\n",
    "# Cast to int or float everything that should be a number\n",
    "X['NumStates'] = X['NumStates'].astype(int)\n",
    "X['AvgDepth'] = X['AvgDepth'].astype(float)\n",
    "X['MaxDepth'] = X['MaxDepth'].astype(int)\n",
    "X['StandardEvosuiteCoverage'] = X['StandardEvosuiteCoverage'].astype(float)\n",
    "X['StandardSCTUnitCoverage'] = X['StandardSCTUnitCoverage'].astype(float)\n",
    "X['SimplifiedEvosuiteCoverage'] = X['SimplifiedEvosuiteCoverage'].astype(float)\n",
    "X['SimplifiedSCTUnitCoverage'] = X['SimplifiedSCTUnitCoverage'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading numerical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard statechars (4 total)\n",
      "->Mean SCTUnit coverage: 56.25\n",
      "->Num. SCTUnit classes that passed: 4\n",
      "->Num. SCTUnit classes that failed: 0\n",
      "->Num. SCTUnit classes with errors: 0\n",
      "->Num. SCTUnit classes not generated: 0\n",
      "->Num. SCTUnit classes blocked: 0\n",
      "Simplified statechars (4 total)\n",
      "->Mean SCTUnit coverage: 68.0\n",
      "->Num. SCTUnit classes that passed: 4\n",
      "->Num. SCTUnit classes that failed: 0\n",
      "->Num. SCTUnit classes with errors: 0\n",
      "->Num. SCTUnit classes not generated: 0\n",
      "->Num. SCTUnit classes blocked: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "standard = len(X)\n",
    "standardPassed = len(X[X['StandardSCTUnitStatus'] == 'PASSED'])\n",
    "standardFailed = len(X[X['StandardSCTUnitStatus'] == 'FAILED'])\n",
    "standardErrors = len(X[X['StandardSCTUnitStatus'] == 'ERRORS'])\n",
    "standardNotGenerated = len(X[X['StandardSCTUnitStatus'] == 'NOT GENERATED'])\n",
    "standardBlocked = len(X[X['StandardSCTUnitStatus'] == 'BLOCKED'])\n",
    "\n",
    "simplified = len(X)\n",
    "simplifiedPassed = len(X[X['SimplifiedSCTUnitStatus'] == 'PASSED'])\n",
    "simplifiedFailed = len(X[X['SimplifiedSCTUnitStatus'] == 'FAILED'])\n",
    "simplifiedErrors = len(X[X['SimplifiedSCTUnitStatus'] == 'ERRORS'])\n",
    "simplifiedNotGenerated = len(X[X['SimplifiedSCTUnitStatus'] == 'NOT GENERATED'])\n",
    "simplifiedBlocked = len(X[X['SimplifiedSCTUnitStatus'] == 'BLOCKED'])\n",
    "\n",
    "# Collecting general data\n",
    "print(f\"Standard statechars ({standard} total)\")\n",
    "print(f\"->Mean SCTUnit coverage: {statistics.mean(X['StandardSCTUnitCoverage'])}\")\n",
    "print(f\"->Num. SCTUnit classes that passed: {standardPassed}\")\n",
    "print(f\"->Num. SCTUnit classes that failed: {standardFailed}\")\n",
    "print(f\"->Num. SCTUnit classes with errors: {standardErrors}\")\n",
    "print(f\"->Num. SCTUnit classes not generated: {standardNotGenerated}\")\n",
    "print(f\"->Num. SCTUnit classes blocked: {standardBlocked}\")\n",
    "\n",
    "# Collecting general data\n",
    "print(f\"Simplified statechars ({simplified} total)\")\n",
    "print(f\"->Mean SCTUnit coverage: {statistics.mean(X['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"->Num. SCTUnit classes that passed: {simplifiedPassed}\")\n",
    "print(f\"->Num. SCTUnit classes that failed: {simplifiedFailed}\")\n",
    "print(f\"->Num. SCTUnit classes with errors: {simplifiedErrors}\")\n",
    "print(f\"->Num. SCTUnit classes not generated: {simplifiedNotGenerated}\")\n",
    "print(f\"->Num. SCTUnit classes blocked: {simplifiedBlocked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient between SimplifiedEvosuiteCoverage and SimplifiedSCTUnitCoverage: 0.3991761088746069\n",
      "Correlation coefficient between SimplifiedSCTUnitCoverage and NumStates: 0.4243083962980371\n",
      "Correlation coefficient between SimplifiedSCTUnitCoverage and AvgDepth: 0.5610151285740982\n",
      "Correlation coefficient between SimplifiedSCTUnitCoverage and MaxDepth: 0.5610151285740981\n"
     ]
    }
   ],
   "source": [
    "# Calculating correlation coefficient\n",
    "correlation = X['SimplifiedEvosuiteCoverage'].corr(X['SimplifiedSCTUnitCoverage'])\n",
    "print(f\"Correlation coefficient between SimplifiedEvosuiteCoverage and SimplifiedSCTUnitCoverage: {correlation}\")\n",
    "correlation = X['SimplifiedSCTUnitCoverage'].corr(X['NumStates'])\n",
    "print(f\"Correlation coefficient between SimplifiedSCTUnitCoverage and NumStates: {correlation}\")\n",
    "correlation = X['SimplifiedSCTUnitCoverage'].corr(X['AvgDepth'])\n",
    "print(f\"Correlation coefficient between SimplifiedSCTUnitCoverage and AvgDepth: {correlation}\")\n",
    "correlation = X['SimplifiedSCTUnitCoverage'].corr(X['MaxDepth'])\n",
    "print(f\"Correlation coefficient between SimplifiedSCTUnitCoverage and MaxDepth: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are useless for clustering\n",
    "drop_features = [\n",
    "    'StandardEvosuiteCoverage', \n",
    "    'StandardSCTUnitCoverage', \n",
    "    'StandardSCTUnitStatus', \n",
    "    'SimplifiedEvosuiteCoverage', \n",
    "    'SimplifiedSCTUnitStatus'\n",
    "]\n",
    "X = X.drop(drop_features, axis=1)\n",
    "\n",
    "# Specify the columns you want to scale\n",
    "columns_to_scale = ['NumStates', 'AvgDepth', 'MaxDepth']  # replace with your column names\n",
    "\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "X_scaled = X.copy()\n",
    "\n",
    "# Scale the specified columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min_cluster_size must be no greater than the number of samples (4). Got 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the OPTICS algorithm using the scaled columns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m optics_model \u001b[38;5;241m=\u001b[39m OPTICS(xi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43moptics_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_scale\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply the DBSCAN algorithm using the scaled columns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dbscan_model \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.55\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_optics.py:367\u001b[0m, in \u001b[0;36mOPTICS.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Extract clusters from the calculated orders and reachability\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 367\u001b[0m     labels_, clusters_ \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_optics_xi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreachability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreachability_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredecessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredecessor_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mordering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordering_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredecessor_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredecessor_correction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_hierarchy_ \u001b[38;5;241m=\u001b[39m clusters_\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbscan\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_optics.py:907\u001b[0m, in \u001b[0;36mcluster_optics_xi\u001b[1;34m(reachability, predecessor, ordering, min_samples, min_cluster_size, xi, predecessor_correction)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_cluster_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    906\u001b[0m     min_cluster_size \u001b[38;5;241m=\u001b[39m min_samples\n\u001b[1;32m--> 907\u001b[0m \u001b[43m_validate_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_cluster_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_cluster_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    909\u001b[0m     min_cluster_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mint\u001b[39m(min_cluster_size \u001b[38;5;241m*\u001b[39m n_samples))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\cluster\\_optics.py:401\u001b[0m, in \u001b[0;36m_validate_size\u001b[1;34m(size, n_samples, param_name)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_size\u001b[39m(size, n_samples, param_name):\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be no greater than the number of samples (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m). Got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m%\u001b[39m (param_name, n_samples, size)\n\u001b[0;32m    404\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: min_cluster_size must be no greater than the number of samples (4). Got 10"
     ]
    }
   ],
   "source": [
    "# Apply the OPTICS algorithm using the scaled columns\n",
    "optics_model = OPTICS(xi=0.1, min_cluster_size=10, min_samples=4)\n",
    "optics_model.fit(X_scaled[columns_to_scale])\n",
    "\n",
    "# Apply the DBSCAN algorithm using the scaled columns\n",
    "dbscan_model = DBSCAN(eps=0.55, min_samples=2)\n",
    "dbscan_model.fit(X_scaled[columns_to_scale])\n",
    "\n",
    "# Apply the KMeans algorithm using the scaled columns\n",
    "kmeans_model = KMeans(n_clusters=4, random_state=0, n_init=\"auto\")\n",
    "kmeans_model.fit(X_scaled[columns_to_scale])\n",
    "\n",
    "# Change labels according to improve readability a posteriori\n",
    "for i in range(len(dbscan_model.labels_)):\n",
    "    val = dbscan_model.labels_[i]\n",
    "    if val == 4:\n",
    "        dbscan_model.labels_[i] = 0 \n",
    "    elif val == 1:\n",
    "        dbscan_model.labels_[i] = 1 \n",
    "    elif val == 0:\n",
    "        dbscan_model.labels_[i] = 2\n",
    "    elif val == 3:\n",
    "        dbscan_model.labels_[i] = 3\n",
    "    elif val == 2:\n",
    "        dbscan_model.labels_[i] = 4\n",
    "\n",
    "# Adding cluster labels to the original DataFrame\n",
    "X['optics_label'] = optics_model.labels_\n",
    "X['dbscan_label'] = dbscan_model.labels_\n",
    "X['kmeans_label'] = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score for each model\n",
    "silhouette_optics = silhouette_score(X_scaled[columns_to_scale], optics_model.labels_)\n",
    "silhouette_dbscan = silhouette_score(X_scaled[columns_to_scale], dbscan_model.labels_)\n",
    "silhouette_kmeans = silhouette_score(X_scaled[columns_to_scale], kmeans_model.labels_)\n",
    "print(\"PREFERE HIGH SILHOUTTE\")\n",
    "print(f\"Silhouette Score (OPTICS): {silhouette_optics}\")\n",
    "print(f\"Silhouette Score (DBSCAN): {silhouette_dbscan}\")\n",
    "print(f\"Silhouette Score (KMeans): {silhouette_kmeans}\")\n",
    "\n",
    "# Calculate Davies-Bouldin index for each model\n",
    "db_index_optics = davies_bouldin_score(X_scaled[columns_to_scale], optics_model.labels_)\n",
    "db_index_dbscan = davies_bouldin_score(X_scaled[columns_to_scale], dbscan_model.labels_)\n",
    "db_index_kmeans = davies_bouldin_score(X_scaled[columns_to_scale], kmeans_model.labels_)\n",
    "print(\"PREFERE LOW DAVIES-BOULDIN INDEX\")\n",
    "print(f\"Davies-Bouldin Index (OPTICS): {db_index_optics}\")\n",
    "print(f\"Davies-Bouldin Index (DBSCAN): {db_index_dbscan}\")\n",
    "print(f\"Davies-Bouldin Index (KMeans): {db_index_kmeans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of outliers in DBSCAN and number of statecharts with max coverage\n",
    "n_outliers = len(X[(X['dbscan_label'] == -1)])\n",
    "print(f\"\\nNumbers of outliers in DBSCAN: {n_outliers}\")\n",
    "n_max_coverage = len(X[(X['SimplifiedSCTUnitCoverage'] == 1.00)])\n",
    "print(f\"Numbers of completely covered statecharts: {n_max_coverage}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about each cluster\n",
    "print(\"CLUSTER 0\")\n",
    "cluster_0 = X[(X['dbscan_label'] ==  0)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_0['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_0['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_0['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_0['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_0)}\")\n",
    "print(f\"Max number of states: {max(cluster_0['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 100% coverage: {len(cluster_0[(cluster_0['SimplifiedSCTUnitCoverage'] ==  1.00)])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_0[(cluster_0['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#print(cluster_0)\n",
    "#with open(\"cluster_0.txt\", \"w\") as f:\n",
    "#  for stc in cluster_0['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)\n",
    "\n",
    "print(\"CLUSTER 1\")\n",
    "cluster_1 = X[(X['dbscan_label'] ==  1)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_1['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_1['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_1['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_1['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_1)}\")\n",
    "print(f\"Max number of states: {max(cluster_1['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 100% coverage: {len(cluster_1[(cluster_1['SimplifiedSCTUnitCoverage'] ==  1.00)])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_1[(cluster_1['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#print(cluster_1)\n",
    "#with open(\"cluster_1.txt\", \"w\") as f:\n",
    "#  for stc in cluster_1['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)\n",
    "\n",
    "print(\"CLUSTER 2\")\n",
    "cluster_2 = X[(X['dbscan_label'] ==  2)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_2['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_2['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_2['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_2['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_2)}\")\n",
    "print(f\"Max number of states: {max(cluster_2['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_2[(cluster_2['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#rint(cluster_2)\n",
    "#with open(\"cluster_2.txt\", \"w\") as f:\n",
    "#  for stc in cluster_2['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)\n",
    "\n",
    "print(\"CLUSTER 3\")\n",
    "cluster_3 = X[(X['dbscan_label'] ==  3)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_3['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_3['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_3['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_3['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_3)}\")\n",
    "print(f\"Max number of states: {max(cluster_3['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_3[(cluster_3['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#print(cluster_3)\n",
    "#with open(\"cluster_3.txt\", \"w\") as f:\n",
    "#  for stc in cluster_3['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)\n",
    "\n",
    "print(\"CLUSTER 4\")\n",
    "cluster_4 = X[(X['dbscan_label'] ==  4)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_4['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_4['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_4['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_4['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_4)}\")\n",
    "print(f\"Max number of states: {max(cluster_4['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_4[(cluster_4['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#print(cluster_4)\n",
    "#with open(\"cluster_4.txt\", \"w\") as f:\n",
    "#  for stc in cluster_4['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)\n",
    "\n",
    "print(\"CLUSTER -1\")\n",
    "cluster_m1 = X[(X['dbscan_label'] ==  -1)]\n",
    "print(f\"Mean coverage: {statistics.mean(cluster_m1['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Median coverage: {statistics.median(cluster_m1['SimplifiedSCTUnitCoverage'])}\")\n",
    "print(f\"Mean NumStates: {statistics.mean(cluster_m1['NumStates'])}\")\n",
    "print(f\"Mean AvgDepth: {statistics.mean(cluster_m1['AvgDepth'])}\")\n",
    "print(f\"Number of statecharts: {len(cluster_m1)}\")\n",
    "print(f\"Max number of states: {max(cluster_m1['NumStates'])}\")\n",
    "print(f\"Number of statecharts with 0% coverage: {len(cluster_m1[(cluster_m1['SimplifiedSCTUnitCoverage'] ==  0.00)])}\")\n",
    "#print(cluster_m1)\n",
    "#with open(\"cluster_m1.txt\", \"w\") as f:\n",
    "#  for stc in cluster_m1['Statechart']:\n",
    "#    print(stc+\"SimplifiedTest,\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for KMeans 3D\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cluster_label in range(kmeans_model.n_clusters):\n",
    "    cluster_mask = (kmeans_model.labels_ == cluster_label)\n",
    "    ax.scatter(X_scaled.loc[cluster_mask, 'NumStates'], \n",
    "                X_scaled.loc[cluster_mask, 'AvgDepth'], \n",
    "                X_scaled.loc[cluster_mask, 'MaxDepth'], \n",
    "                label=f'Cluster {cluster_label}')\n",
    "ax.scatter(kmeans_model.cluster_centers_[:, 0], \n",
    "            kmeans_model.cluster_centers_[:, 1], \n",
    "            kmeans_model.cluster_centers_[:, 2], \n",
    "            marker='x', color='black', label='Centroids')\n",
    "ax.set_title('KMeans Clustering')\n",
    "ax.set_xlabel('NumStates (scaled)')\n",
    "ax.set_ylabel('AvgDepth (scaled)')\n",
    "ax.set_zlabel('MaxDepth (scaled)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate colors for each DBSCAN cluster\n",
    "unique_labels = np.unique(dbscan_model.labels_)\n",
    "colors = sns.color_palette('bright', len(unique_labels))\n",
    "\n",
    "# Plot for DBSCAN in 3D\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cluster_label, color in zip(unique_labels, colors):\n",
    "    cluster_mask = (dbscan_model.labels_ == cluster_label)\n",
    "    ax.scatter(X_scaled.loc[cluster_mask, 'NumStates'], \n",
    "                X_scaled.loc[cluster_mask, 'AvgDepth'], \n",
    "                X_scaled.loc[cluster_mask, 'MaxDepth'], \n",
    "                label=f'Cluster {cluster_label}', color=color)\n",
    "ax.set_title('DBSCAN Clustering', fontsize=18)\n",
    "ax.set_xlabel('NumStates (scaled)', fontsize=14)\n",
    "ax.set_ylabel('AvgDepth (scaled)', fontsize=14)\n",
    "ax.set_zlabel('MaxDepth (scaled)', fontsize=14)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for DBSCAN in 2D view (dropping AvgDepth)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "for cluster_label in np.unique(dbscan_model.labels_):\n",
    "    cluster_mask = (dbscan_model.labels_ == cluster_label)\n",
    "    ax.scatter(X_scaled.loc[cluster_mask, 'NumStates'], \n",
    "                X_scaled.loc[cluster_mask, 'MaxDepth'], \n",
    "                label=f'Cluster {cluster_label}')\n",
    "ax.set_title('DBSCAN Clustering (2D view)', fontsize=16)\n",
    "ax.set_xlabel('NumStates (scaled)', fontsize=14)\n",
    "ax.set_ylabel('MaxDepth (scaled)', fontsize=14)\n",
    "ax.legend()\n",
    "ax = fig.add_subplot(132)\n",
    "for cluster_label in np.unique(dbscan_model.labels_):\n",
    "    cluster_mask = (dbscan_model.labels_ == cluster_label)\n",
    "    ax.scatter(X_scaled.loc[cluster_mask, 'AvgDepth'], \n",
    "                X_scaled.loc[cluster_mask, 'MaxDepth'], \n",
    "                label=f'Cluster {cluster_label}')\n",
    "ax.set_title('DBSCAN Clustering (2D view)', fontsize=16)\n",
    "ax.set_xlabel('AvgDepth (scaled)', fontsize=14)\n",
    "ax.set_ylabel('MaxDepth (scaled)', fontsize=14)\n",
    "ax.legend()\n",
    "ax = fig.add_subplot(133)\n",
    "for cluster_label in np.unique(dbscan_model.labels_):\n",
    "    cluster_mask = (dbscan_model.labels_ == cluster_label)\n",
    "    ax.scatter(X_scaled.loc[cluster_mask, 'NumStates'], \n",
    "                X_scaled.loc[cluster_mask, 'AvgDepth'], \n",
    "                label=f'Cluster {cluster_label}')\n",
    "ax.set_title('DBSCAN Clustering (2D view)', fontsize=16)\n",
    "ax.set_xlabel('NumStates (scaled)', fontsize=14)\n",
    "ax.set_ylabel('AvgDepth (scaled)', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage Analysis for DBSCAN\n",
    "# Analyze coverage within DBSCAN clusters\n",
    "cluster_coverage_stats = X.groupby('dbscan_label')['SimplifiedSCTUnitCoverage'].describe()\n",
    "# Create a custom color palette for the violin plot with string keys\n",
    "palette = {str(label): color for label, color in zip(unique_labels, colors)}\n",
    "# Sort the data by 'dbscan_label' in ascending order\n",
    "X_sorted = X.sort_values(by='dbscan_label')\n",
    "# Convert 'dbscan_label' to string type (for the palette)\n",
    "X['dbscan_label'] = X['dbscan_label'].astype(str)\n",
    "# Visualize coverage distribution within a violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='dbscan_label', y='SimplifiedSCTUnitCoverage', data=X_sorted, palette=palette)\n",
    "plt.ylim(0, 1)  # Limit y-axis from 0 to 1\n",
    "plt.title('Coverage Distribution across DBSCAN Clusters', fontsize=30)\n",
    "plt.xlabel('DBSCAN Cluster Label', fontsize=20)\n",
    "plt.ylabel('SCTUnit Coverage', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)  # Add grid lines for better readability\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
